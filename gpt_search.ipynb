{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googlesearch-python\n",
    "!pip install requests\n",
    "from googlesearch import search\n",
    "import openai\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt():\n",
    "    your_question = input(\"Input what you are curious about: \")\n",
    "    with open(\"openai_api_key.txt\", \"r\") as f:\n",
    "        openai.api_key = f.read().strip()\n",
    "\n",
    "    query1 = []\n",
    "    google_it = f\"What are the key terms you can extract from this query that, if searched, will help find the answers to {your_question}\"\n",
    "    model1 = \"text-davinci-003\"\n",
    "    params1 = {\n",
    "        \"prompt\": google_it,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    }\n",
    "    query1 = openai.Completion.create(engine=model1, **params1)\n",
    "    print(query1.choices[0].text.strip())\n",
    "    google_it = f\"Return exactly one Google search query, phrased as a question, I could Google to find information regarding {your_question}\"\n",
    "    model1 = \"text-davinci-003\"\n",
    "    params1 = {\n",
    "        \"prompt\": google_it,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    }\n",
    "    query2 = openai.Completion.create(engine=model1, **params1)\n",
    "    query2\n",
    "\n",
    "    return query1, query2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query1, query2):\n",
    "    query = query2.choices[0].text.strip()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_google(query):\n",
    "    # Iterate over the search results and append each URL to the list\n",
    "    # programmatically search Google\n",
    "    import requests\n",
    "    import sys\n",
    "    import os\n",
    "    def open_file(filepath):\n",
    "        with open(filepath, 'r', encoding='latin-1') as infile:\n",
    "            return infile.read()\n",
    "    os.chdir('/Path/gpt_search')\n",
    "\n",
    "    # API KEY from: https://developers.google.com/custom-search/v1/overview\n",
    "    API_KEY = open_file('/Path/gpt_search/google_api_key.txt')\n",
    "    # get your Search Engine ID on your CSE control panel\n",
    "    SEARCH_ENGINE_ID = open_file('/Path/gpt_search/google_searchengine_id.txt')\n",
    "    print(API_KEY)\n",
    "    print(SEARCH_ENGINE_ID)\n",
    "    try:\n",
    "        page = int(sys.argv[2])\n",
    "        # make sure page is positive\n",
    "        assert page > 0\n",
    "    except:\n",
    "        print(\"Page number isn't specified, defaulting to 1\")\n",
    "        page = 1\n",
    "    # constructing the URL\n",
    "    # doc: https://developers.google.com/custom-search/v1/using_rest\n",
    "    # calculating start, (page=2) => (start=11), (page=3) => (start=21)\n",
    "    # for some reason it's best to copy paste the query here\n",
    "    # query = \"{query}\"\n",
    "    print(query)\n",
    "    start = (page - 1) * 10 + 1\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?cx={SEARCH_ENGINE_ID}&key={API_KEY}&q={query}\"\n",
    "    print(url)\n",
    "    # make the API request\n",
    "    data = requests.get(url).json()\n",
    "    # get the result items\n",
    "    search_items = data.get(\"items\")\n",
    "    print(search_items)\n",
    "    if search_items is None:\n",
    "        print(\"No search results found\")\n",
    "        exit()\n",
    "    # iterate over 10 results found\n",
    "    links = []\n",
    "    for i, search_item in enumerate(search_items, start=1):\n",
    "        try:\n",
    "            long_description = search_item[\"pagemap\"][\"metatags\"][0][\"og:description\"]\n",
    "        except KeyError:\n",
    "            long_description = \"N/A\"\n",
    "        # get the page title\n",
    "        title = search_item.get(\"title\")\n",
    "        # page snippet\n",
    "        snippet = search_item.get(\"snippet\")\n",
    "        # alternatively, you can get the HTML snippet (bolded keywords)\n",
    "        html_snippet = search_item.get(\"htmlSnippet\")\n",
    "        # extract the page url\n",
    "        link = search_item.get(\"link\")\n",
    "        links.append(link)\n",
    "        # print the results\n",
    "        print(\"=\"*10, f\"Result #{i+start-1}\", \"=\"*10)\n",
    "        print(\"Title:\", title)\n",
    "        print(\"Description:\", snippet)\n",
    "        print(\"Long description:\", long_description)\n",
    "        print(\"URL:\", link, \"\\n\")\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_results(links):\n",
    "    import openai\n",
    "    from bs4 import BeautifulSoup\n",
    "    with open(\"openai_api_key.txt\", \"r\") as f:\n",
    "        openai.api_key = f.read().strip()\n",
    "\n",
    "    # Cycle through the urls to get the summarised answer\n",
    "    answer = []\n",
    "    num_results = 3\n",
    "\n",
    "    for x in range(num_results):\n",
    "        # Get the page content\n",
    "        html = requests.get(links[x]).text\n",
    "\n",
    "        # Extract the page title and text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        title = soup.title.string\n",
    "        text = \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n",
    "\n",
    "        # Set up the GPT-3 API request\n",
    "        request = f\"Please summarize the following article:\\nTitle: {title}\\n\\n{text[:4000]}\"\n",
    "        model = \"text-davinci-003\"\n",
    "        params = {\n",
    "            \"prompt\": request,\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 100,\n",
    "            \"top_p\": 1,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0\n",
    "        }\n",
    "        \n",
    "        # Call the GPT-3 API and print the response\n",
    "        response = openai.Completion.create(engine=model, **params)\n",
    "        summary = response.choices[0].text.strip()\n",
    "        answer.append(summary)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the output\n",
    "# Define function to clean text using GPT-3 API\n",
    "def clean_text(t):    \n",
    "    # Define parameters for GPT-3 API\n",
    "    max_tokens = 2000\n",
    "    temperature = 0.7\n",
    "    stop = \"\\n\\n\"\n",
    "    # Split text into smaller chunks and process each chunk separately\n",
    "    chunk_size = 2000\n",
    "    chunks = [t[i:i+chunk_size] for i in range(0, len(t), chunk_size)]\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Generate cleaned text using GPT-3 API\n",
    "        # Define prompt for GPT-3 API\n",
    "        prompt = (f\"Rewrite the following text taking away all weird characters:\\n\\n{chunk}\\n\\n\"\n",
    "              \"The cleaned text is:\")\n",
    "        print(prompt)\n",
    "        response = openai.ChatCompletion.create(\n",
    "                model = \"gpt-3.5-turbo\",\n",
    "                messages = [\n",
    "                    #{\"role\": \"system\", \"content\" : \"You are able to perfectly clean and replicate text\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "        print(response)\n",
    "        cleaned_chunk = response['choices'][0]['message']['content']\n",
    "        print(cleaned_chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "\n",
    "    # Join cleaned chunks into a single string and return it\n",
    "    cleaned_text = \"\".join(cleaned_chunks)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the questions\n",
    "q1, q2 = ask_gpt()\n",
    "query_clean = clean_query(q1, q2)\n",
    "google_links = iterate_google(query_clean)\n",
    "text_scraped = scrape_results(google_links)\n",
    "# Convert to a string\n",
    "text_string = ' '.join(text_scraped)\n",
    "cleaned_text = clean_text(text_string)\n",
    "print(cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
